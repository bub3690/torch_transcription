{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Speech Recognition with Wav2Vec2\n",
    "================================\n",
    "\n",
    "**Author**: `Moto Hira <moto@fb.com>`__\n",
    "\n",
    "**번역자** : `이종법 <bub3690@naver.com>`__\n",
    "\n",
    "\n",
    "\n",
    "번역문 :\n",
    "\n",
    "이 튜토리얼은 미리 학습된(pre-trained) wav2vec 2.0 모델을 사용하여 어떻게 음성 인식을 수행하는지 보여줍니다.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "원문:\n",
    "\n",
    "This tutorial shows how to perform speech recognition using using\n",
    "pre-trained models from wav2vec 2.0\n",
    "[`paper <https://arxiv.org/abs/2006.11477>`__].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "--------\n",
    "\n",
    "\n",
    "\n",
    "번역문 :\n",
    "\n",
    "음성 인식의 과정은 다음과 유사합니다.\n",
    "\n",
    "1. 오디오 파형으로부터 음향 특징을 추출한다.\n",
    "\n",
    "2. 각 프레임별로 음향 특징의 범주를 예측한다. \n",
    "\n",
    "3. 연속된 범주 확률들로 부터 가설을 생성한다.\n",
    "\n",
    "\n",
    "Torchaudio는 미리 학습된 가중치들과 예상되는 샘플링 비율, 범주 레이블과 같은 관련 정보에 쉽게 접근하게 해줍니다.\n",
    "그것들은 함께 묶여, :py:func:`torchaudio.pipelines` 모듈에서 사용가능합니다.\n",
    "\n",
    "---\n",
    "\n",
    "원문 :\n",
    "\n",
    "\n",
    "The process of speech recognition looks like the following.\n",
    "\n",
    "1. Extract the acoustic features from audio waveform\n",
    "\n",
    "2. Estimate the class of the acoustic features frame-by-frame\n",
    "\n",
    "3. Generate hypothesis from the sequence of the class probabilities\n",
    "\n",
    "Torchaudio provides easy access to the pre-trained weights and\n",
    "associated information, such as the expected sample rate and class\n",
    "labels. They are bundled together and available under\n",
    ":py:func:`torchaudio.pipelines` module.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation\n",
    "-----------\n",
    "번역문:\n",
    "\n",
    "가장 먼저 필수 패키지를  불러오고, 우리가 작업할 데이터를 받아옵니다.\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "First we import the necessary packages, and fetch data that we work on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(device)\n",
    "\n",
    "SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"  # noqa: E501\n",
    "SPEECH_FILE = \"_assets/speech.wav\"\n",
    "\n",
    "if not os.path.exists(SPEECH_FILE):\n",
    "    os.makedirs(\"_assets\", exist_ok=True)\n",
    "    with open(SPEECH_FILE, \"wb\") as file:\n",
    "        file.write(requests.get(SPEECH_URL).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline\n",
    "-------------------\n",
    "\n",
    "번역문 :\n",
    "\n",
    "가장 먼저, 특징 추출과 분류를 수행하는 Wav2Vec2 모델을 생성합니다.\n",
    "\n",
    "torchaudio 에서 미리 학습된 Wav2Vec2 가중치들은 두가지가 있습니다. 하나는 자동 음성 인식(ASR)에 미리 학습된 것, 다른 하나는 미리 학습되지 않은 것입니다.\n",
    "\n",
    "Wav2Vec2 ( 그리고 HuBERT) 모델들은 자기-지도학습 방법으로 학습됩니다. 그 모델들은 가장 먼저 표현 학습만을 위해 오디오를 사용하여 학습되고, 그리고 나서 추가적인 레이블로 특정 업무를 미세조정합니다.\n",
    "\n",
    "또한 미세 조정없이 미리 학습된 가중치들은 다른 적용하려는 업무(downstream task)를 미세조정 할 수 있습니다, 그러나 이 튜토리얼은 그것을 다루지 않습니다.\n",
    "\n",
    "우리는 이 여기서 :py:func:`torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H` 을 사용할 것입니다.\n",
    "\n",
    ":py:mod:`torchaudio.pipelines` 과 같은 많은 모델들이 제공됩니다. 어떻게 학습되는지 자세한 방법은 문서를 참조해주세요.\n",
    "\n",
    "묶음(bundle) 객체들은 모델을 객체를 생성하는 인터페이스와 다른 정보들을 제공합니다. 샘플링 비율과 범주 레이블은 다음과 같이 볼 수 있습니다. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "First, we will create a Wav2Vec2 model that performs the feature\n",
    "extraction and the classification.\n",
    "\n",
    "There are two types of Wav2Vec2 pre-trained weights available in\n",
    "torchaudio. The ones fine-tuned for ASR task, and the ones not\n",
    "fine-tuned.\n",
    "\n",
    "Wav2Vec2 (and HuBERT) models are trained in self-supervised manner. They\n",
    "are firstly trained with audio only for representation learning, then\n",
    "fine-tuned for a specific task with additional labels.\n",
    "\n",
    "The pre-trained weights without fine-tuning can be fine-tuned\n",
    "for other downstream tasks as well, but this tutorial does not\n",
    "cover that.\n",
    "\n",
    "We will use :py:func:`torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H` here.\n",
    "\n",
    "There are multiple models available as\n",
    ":py:mod:`torchaudio.pipelines`. Please check the documentation for\n",
    "the detail of how they are trained.\n",
    "\n",
    "The bundle object provides the interface to instantiate model and other\n",
    "information. Sampling rate and the class labels are found as follow.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "\n",
    "print(\"Labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 :\n",
    "\n",
    "모델은 다음과 같이 구축될 수 있습니다. 이 과정은 자동으로 미리 학습된 가중치를 불러오고, 모델에 탑재 해줍니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 :\n",
    "\n",
    "Model can be constructed as following. This process will automatically\n",
    "fetch the pre-trained weights and load it into the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bundle.get_model().to(device)\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data\n",
    "------------\n",
    "\n",
    "번역문 :\n",
    "\n",
    "우리는 `VOiCES dataset <https://iqtlabs.github.io/voices/>`__,의 음성 데이터를 사용할 것 입니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 :\n",
    "\n",
    "We will use the speech data from `VOiCES\n",
    "dataset <https://iqtlabs.github.io/voices/>`__, which is licensed under\n",
    "Creative Commos BY 4.0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 : \n",
    "\n",
    "데이터를 탑재 하기위해서는, :py:func:`torchaudio.load`을 사용합니다.\n",
    "\n",
    "만약 샘플링 비율이 파이프라인(pipeline)이 예상하는 것과 다르다면, :py:func:`torchaudio.functional.resample` 을 사용하여 다시 샘플링 할 수 있습니다.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>추가 정보</h4><p>- :py:func:`torchaudio.functional.resample` CUDA 텐서에서도 사용 가능합니다.\n",
    "   - 같은 샘플링 비율들의 집합을 여러번 다시 샘플링 하는 경우에는,\n",
    "     :py:func:`torchaudio.transforms.Resample`을 사용하는 것이 성능을 더 높여줄 수 있습니다.</p></div>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "\n",
    "To load data, we use :py:func:`torchaudio.load`.\n",
    "\n",
    "If the sampling rate is different from what the pipeline expects, then\n",
    "we can use :py:func:`torchaudio.functional.resample` for resampling.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>- :py:func:`torchaudio.functional.resample` works on CUDA tensors as well.\n",
    "   - When performing resampling multiple times on the same set of sample rates,\n",
    "     using :py:func:`torchaudio.transforms.Resample` might improve the performace.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting acoustic features\n",
    "----------------------------\n",
    "\n",
    "번역문 : \n",
    "\n",
    "다음 단계는 오디오로 부터 음향 특징을 추출하는 것입니다.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>추가 정보</h4><p>자동-음성-인식(ASR) 업무에 미세조정된 Wav2Vec2 모델은 특징 추출과 분류를 한번에 수행가능하지만, 튜토리얼을 위해, 여기서는 어떻게 특징 추출하는지 보여드립니다.</p></div>\n",
    "\n",
    "---\n",
    "\n",
    "원문 :\n",
    "\n",
    "The next step is to extract acoustic features from the audio.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Wav2Vec2 models fine-tuned for ASR task can perform feature\n",
    "   extraction and classification with one step, but for the sake of the\n",
    "   tutorial, we also show how to perform feature extraction here.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features, _ = model.extract_features(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 :\n",
    "\n",
    "특징들은 텐서의 리스트로 반환됩니다. 각 텐서는 트랜스폼 계층(Transform layer)의 출력입니다. \n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "The returned features is a list of tensors. Each tensor is the output of\n",
    "a transformer layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(features), 1, figsize=(16, 4.3 * len(features)))\n",
    "for i, feats in enumerate(features):\n",
    "    ax[i].imshow(feats[0].cpu())\n",
    "    ax[i].set_title(f\"Feature from transformer layer {i+1}\")\n",
    "    ax[i].set_xlabel(\"Feature dimension\")\n",
    "    ax[i].set_ylabel(\"Frame (time-axis)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature classification\n",
    "----------------------\n",
    "\n",
    "번역문 : \n",
    "\n",
    "음향 특징들이 추출되면, 다음 단계는 특징들을 범주들의 집합으로 분류하는 것입니다.\n",
    "\n",
    "Wav2Vec2 모델은 특징 추출과 분류를 한번에 수행하는 방법을 제공합니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 :\n",
    "\n",
    "Once the acoustic features are extracted, the next step is to classify\n",
    "them into a set of categories.\n",
    "\n",
    "Wav2Vec2 model provides method to perform the feature extraction and\n",
    "classification in one step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission, _ = model(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 : \n",
    "\n",
    "출력값은 로짓의 형태입니다. 이것은 확률의 형태가 아닙니다.\n",
    "\n",
    "이것을 시각화 해봅시다.\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "The output is in the form of logits. It is not in the form of\n",
    "probability.\n",
    "\n",
    "Let’s visualize this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(emission[0].cpu().T)\n",
    "plt.title(\"Classification result\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.show()\n",
    "print(\"Class labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 : \n",
    "\n",
    "시간 축을 따라 특정 레이블에 강한 표시들을 볼 수 있습니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "We can see that there are strong indications to certain labels across\n",
    "the time line.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating transcripts\n",
    "----------------------\n",
    "번역문 : \n",
    "\n",
    "연속된 레이블 확률들로 부터, 이제 우리는 글을 생성하고 싶습니다. 가설을 생성하는 과정은 종종 \"복호화(decoding)\" 이라고도 불립니다.\n",
    "\n",
    "\n",
    "복호화는 단순 분류보다 더 정교한데, 왜냐하면 특정 시간 단계에서 복호화 하는 것은 주변 관측 값들에 영향을 받기 때문입니다.\n",
    "\n",
    "``night``와 ``knight``를 예로 들자. 비록 그들의 사전 확률분포는 다르지만 (전형적인 대화에서 ``night``가 더 많이 등장한다.), ``a knight with a sword``와 같이 ``knight``가 있는 글을 정확히 생성하기 위해서, 복호화 작업은 충분한 문맥을 보기전까지  최종 결정을 연기해야한다.  \n",
    "\n",
    "많은 복호화 기술들이 제안되었고, 그 기술들은 단어 사전이나 언어 모델(language model) 처럼 외부 자료를 필요로 합니다.\n",
    "\n",
    "이 튜토리얼에서는, 단순화를 위해, 탐욕(Greedy) 복호화를 수행한다. 탐욕 보호화는 그런 외부 요소들에 의존하지않고, 간단히 매 시간 단계마다 가장 최선의 가설을 선택한다. 그러므로, 문맥 정보는 사용되지 않고, 오직 한 글만 생성이 될 수 있다.\n",
    "\n",
    "우리는 탐욕 복호화 알고리즘 정의부터 시작하겠습니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "\n",
    "From the sequence of label probabilities, now we want to generate\n",
    "transcripts. The process to generate hypotheses is often called\n",
    "“decoding”.\n",
    "\n",
    "Decoding is more elaborate than simple classification because\n",
    "decoding at certain time step can be affected by surrounding\n",
    "observations.\n",
    "\n",
    "For example, take a word like ``night`` and ``knight``. Even if their\n",
    "prior probability distribution are differnt (in typical conversations,\n",
    "``night`` would occur way more often than ``knight``), to accurately\n",
    "generate transcripts with ``knight``, such as ``a knight with a sword``,\n",
    "the decoding process has to postpone the final decision until it sees\n",
    "enough context.\n",
    "\n",
    "There are many decoding techniques proposed, and they require external\n",
    "resources, such as word dictionary and language models.\n",
    "\n",
    "In this tutorial, for the sake of simplicity, we will perform greedy\n",
    "decoding which does not depend on such external components, and simply\n",
    "pick up the best hypothesis at each time step. Therefore, the context\n",
    "information are not used, and only one transcript can be generated.\n",
    "\n",
    "We start by defining greedy decoding algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor) -> str:\n",
    "        \"\"\"Given a sequence emission over labels, get the best path string\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          str: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        return \"\".join([self.labels[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 : \n",
    "\n",
    "복호기 객체를 생성하고 글을 복호화 합니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "원문 : \n",
    "\n",
    "\n",
    "Now create the decoder object and decode the transcript.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
    "transcript = decoder(emission[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 : \n",
    "\n",
    "결과를 확인하고 오디오를 다시 들어보세요.\n",
    "\n",
    "---\n",
    "원문 :\n",
    "\n",
    "\n",
    "Let’s check the result and listen again to the audio.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcript)\n",
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역문 : \n",
    "\n",
    "자동-음성-인식(ASR) 모델은 연결주의-시간적-분류(CTC) 손실 함수에 의해 미세조정된다.\n",
    "\n",
    "CTC 손실 함수의 자세한 내용은 `here <https://distill.pub/2017/ctc/>`__에서 설명된다.\n",
    "\n",
    "CTC에서 빈 토큰 (ϵ)은 이전 기호의 반복을 나타내는 특별한 토큰이다. 복호화에서는, 간단히 무시된다. \n",
    "\n",
    "---\n",
    "원문 :\n",
    "\n",
    "The ASR model is fine-tuned using a loss function called Connectionist Temporal Classification (CTC).\n",
    "The detail of CTC loss is explained\n",
    "`here <https://distill.pub/2017/ctc/>`__. In CTC a blank token (ϵ) is a\n",
    "special token which represents a repetition of the previous symbol. In\n",
    "decoding, these are simply ignored.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "번역문 : \n",
    "\n",
    "이 튜토리얼에서는, 우리는 음향 특징 추출, 음성 인식을 위해서 :py:mod:`torchaudio.pipelines`을 어떻게 사용하는지 살펴보았다. 모델을 생성하고 출력을 받는것은 짧게 두줄이다.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "원문 :\n",
    "\n",
    "In this tutorial, we looked at how to use :py:mod:`torchaudio.pipelines` to\n",
    "perform acoustic feature extraction and speech recognition. Constructing\n",
    "a model and getting the emission is as short as two lines.\n",
    "\n",
    "::\n",
    "\n",
    "   model = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.get_model()\n",
    "   emission = model(waveforms, ...)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
